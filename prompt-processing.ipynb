{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":45917,"databundleVersionId":5024308,"sourceType":"competition"},{"sourceId":4988409,"sourceType":"datasetVersion","datasetId":2893282}],"dockerImageVersionId":30446,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Cleansing approach for Diffusion-DB using vector search\n\nThe data cleansing of DiffusionDB-2M has been disclosed in public notebooks and discussions.\n\nhttps://www.kaggle.com/code/shoheiazuma/diffusiondb-data-cleansing/notebook\nhttps://www.kaggle.com/competitions/stable-diffusion-image-to-prompts/discussion/398529\n\nI achieved this through very simple rule-based filtering and filtering based on the similarity of prompt vectors. For the evaluation of vector similarity, I used the faiss vector search library, which is used for recommendation and similar image search.\n\nI believe that this search technique will be important in this competition.","metadata":{}},{"cell_type":"code","source":"!pip install pyrebase4 gdown faiss-gpu\n!gdown 1pF1u8mekNs_z_KvFIJRTdqhlFHm1lp5n","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:31:12.108608Z","iopub.execute_input":"2024-02-19T23:31:12.108953Z","iopub.status.idle":"2024-02-19T23:33:43.412397Z","shell.execute_reply.started":"2024-02-19T23:31:12.108911Z","shell.execute_reply":"2024-02-19T23:33:43.411005Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78973eb77810>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyrebase4/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78973eb77c10>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyrebase4/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78973eb7b050>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyrebase4/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78973eb7b210>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyrebase4/\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x78973eb7b550>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pyrebase4/\u001b[0m\u001b[33m\n\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pyrebase4 (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for pyrebase4\u001b[0m\u001b[31m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0m/bin/bash: gdown: command not found\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport re\nimport requests\nfrom bs4 import BeautifulSoup as Soup\nimport pyrebase\nimport os\n\n\n\nconfig_path = 'firebase_auth.json'\nassert os.path.exists(config_path)\n\nconfig = {\n  \"apiKey\": \"AIzaSyBnWywH3ZswQNyLblBohBAp__f_F2myt5M\",\n  \"authDomain\": \"datasetcollect-81ac0.firebaseapp.com\",\n  \"databaseURL\": \"https://datasetcollect-81ac0-default-rtdb.firebaseio.com\",\n  \"storageBucket\": \"datasetcollect-81ac0.appspot.com\",\n  \"ServiceAccount\":config_path,\n}\n\n\n\nfirebase = pyrebase.initialize_app(config)\ndb = firebase.database()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.415119Z","iopub.execute_input":"2024-02-19T23:33:43.415491Z","iopub.status.idle":"2024-02-19T23:33:43.698652Z","shell.execute_reply.started":"2024-02-19T23:33:43.415454Z","shell.execute_reply":"2024-02-19T23:33:43.695692Z"},"trusted":true},"execution_count":2,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/2553233322.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyrebase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyrebase'"],"ename":"ModuleNotFoundError","evalue":"No module named 'pyrebase'","output_type":"error"}]},{"cell_type":"code","source":"def start_gen():\n  gen_id = db.get('gen_id').val()\n  if not gen_id:\n     raise Exception('Unexpected behaviour, gen_id has to be defined, contact @round_tensor')\n  name, val = gen_id.popitem()\n  if val == 1:\n    raise Exception('Someone is runing gen process right now')\n  elif val != 0:\n    raise Exception('Wtf why gen_id != 1 or 0')\n elif val == 0:\n    db.child(\"gen_id\").set(1)\n\n\nstart_gen()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.699358Z","iopub.status.idle":"2024-02-19T23:33:43.699718Z","shell.execute_reply.started":"2024-02-19T23:33:43.699524Z","shell.execute_reply":"2024-02-19T23:33:43.699548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_LEN = 150\n\n#формат: \"A sticker of {...}\" - без точки на конце.\n\n\n\ndef preprocess(s):\n    s = s.replace(r'\\n', '').replace('\\n', '').strip().capitalize()\n    return s\n\ndef parse_chatgpt_url(url): #тут могут быть ошибки.\n    r = requests.get(url)\n    assert r.ok\n    body = Soup(r.content, 'lxml').find('body')\n    \n    string = body.find_all('script')[1].text\n    prompts = []\n    parts_idx = [m.start() for m in re.finditer(\"parts\", string)]\n    for i in range(len(parts_idx) - 1):\n        text = string[parts_idx[i]: parts_idx[i + 1]][7:]\n        text = text[:text.find('},')]\n        lst = list(re.findall(r\"A sticker of .*?\\\\n\", text, re.DOTALL))\n        last = text[text.rfind('A sticker of'):].strip().capitalize()\n        if last.count(r'\\n') > 0:\n            lst.append(last[:last.find(r'\\n')])\n        else:\n            lst.append(last.replace('\"]', ''))\n        prompts.extend([i for i in [preprocess(q) for q in lst] if i.endswith('.') and i.count('.') == 1])\n\n    return set(prompts)\n\ndef parse_chatgpt_urls(urls):\n    res = set()\n    for url in urls:\n        for p in parse_chatgpt_url(url):\n            res.add(p)\n    return list(res)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.701233Z","iopub.status.idle":"2024-02-19T23:33:43.701621Z","shell.execute_reply.started":"2024-02-19T23:33:43.701427Z","shell.execute_reply":"2024-02-19T23:33:43.701446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"urls = ['https://chat.openai.com/share/46828f0c-0944-463f-a394-b56c07c3249d', 'https://chat.openai.com/share/9708aa83-e06b-4873-8b89-3855149f13d6']\nsint_prompts = parse_chatgpt_urls(urls)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.703426Z","iopub.status.idle":"2024-02-19T23:33:43.703772Z","shell.execute_reply.started":"2024-02-19T23:33:43.703598Z","shell.execute_reply":"2024-02-19T23:33:43.703615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prompts(tabeles_to_parse=['table1']):\n    result = []\n    for table in tabeles_to_parse:\n        resp = db.get(table).val()\n        for table_name, table_data in resp.items():\n            if table_name not in tabeles_to_parse:\n                continue\n            for record_id, record_data in table_data.items():\n                result.append(record_data['prompt_sticker'])\n    return result\n\n\ndb_prompts = get_prompts()\nsint_prompts.extend(db_prompts)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.705427Z","iopub.status.idle":"2024-02-19T23:33:43.705834Z","shell.execute_reply.started":"2024-02-19T23:33:43.705638Z","shell.execute_reply":"2024-02-19T23:33:43.705658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"db_prompts","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.706821Z","iopub.status.idle":"2024-02-19T23:33:43.707209Z","shell.execute_reply.started":"2024-02-19T23:33:43.707016Z","shell.execute_reply":"2024-02-19T23:33:43.707035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sys\nimport re\nimport faiss\nimport torch\nimport numpy as np\nimport polars as pl\nfrom pathlib import Path\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport polars as pl\n\n\nsys.path.append(\"/kaggle/input/sentence-transformers-222/sentence-transformers\")\nfrom sentence_transformers import SentenceTransformer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-19T23:33:43.708471Z","iopub.status.idle":"2024-02-19T23:33:43.708812Z","shell.execute_reply.started":"2024-02-19T23:33:43.708640Z","shell.execute_reply":"2024-02-19T23:33:43.708657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_string(string: str) -> bool:\n    return bool(re.search(r'[^A-Za-z0-9,.\\\\-\\\\s]', string))\n\nsint_prompts = [i.replace('A sticker of', '', 1).strip() for i in sint_prompts if len(i.split()) >= 5 and len(i) <= MAX_LEN and check_string(i)]\n\ndf_sint = pl.DataFrame({'prompt':sint_prompts})\ndf_sint","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.710368Z","iopub.status.idle":"2024-02-19T23:33:43.710958Z","shell.execute_reply.started":"2024-02-19T23:33:43.710625Z","shell.execute_reply":"2024-02-19T23:33:43.710657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vectorize using SentenceTransformers","metadata":{}},{"cell_type":"code","source":"model = SentenceTransformer(\"/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2\")\nvector = model.encode(df_sint[\"prompt\"].to_numpy(), batch_size=512, show_progress_bar=True, device=\"cuda\", convert_to_tensor=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.714925Z","iopub.status.idle":"2024-02-19T23:33:43.715379Z","shell.execute_reply.started":"2024-02-19T23:33:43.715183Z","shell.execute_reply":"2024-02-19T23:33:43.715203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Similarity filtering using vector search","metadata":{}},{"cell_type":"code","source":"threshold = 0.50  # !!!должен быть не оч строгим, т.к red ferrari, green ferrari, black ferrari надо дропнуть, а red ferrari, red ford mustand оставить\n#поставил 0.5 чтобы получить мало промптов для примера, 0.8 нормально\nn_neighbors = 1000  \n\nbatch_size = 1000\nsimilar_vectors = []\n\n\nresources = faiss.StandardGpuResources()\nindex = faiss.IndexIVFFlat(faiss.IndexFlatIP(vector.shape[1]), vector.shape[1], 5, faiss.METRIC_INNER_PRODUCT)\ngpu_index = faiss.index_cpu_to_gpu(resources, 0, index)\n\ngpu_index.train(F.normalize(vector).cpu().numpy())\ngpu_index.add(F.normalize(vector).cpu().numpy())\n\nsim_batches = []\nindices_batches = []\n\nfor i in tqdm(range(0, len(vector), batch_size)):\n    batch_data = vector.cpu().numpy()[i:i + batch_size]\n    similarities, indices = gpu_index.search(batch_data, n_neighbors)\n    sim_batches.append(similarities)\n    indices_batches.append(indices)\n    \n    \n    for j in range(similarities.shape[0]):\n        close_vectors = indices[j, similarities[j] >= threshold]\n        \n        index_base = i\n                \n        close_vectors = close_vectors[close_vectors != index_base + j]  \n        \n        similar_vectors.append((index_base + j, close_vectors))\n\n\n\ndf = df_sint.with_columns(pl.Series(values=list(range(len(df_sint))), name=\"index\"))\ndf = df.filter(~pl.col(\"index\").is_in(np.unique(np.concatenate([x for _, x in similar_vectors])).tolist()))\ndf = df.to_pandas()\ndf['prompt'] = df['prompt'].apply(lambda p: f'A sticker of {p}')\ndf['prompt'] = [i for i in df['prompt'] if i not in db_prompts]\ndf","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.716672Z","iopub.status.idle":"2024-02-19T23:33:43.717062Z","shell.execute_reply.started":"2024-02-19T23:33:43.716854Z","shell.execute_reply":"2024-02-19T23:33:43.716871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display similar prompts which >= threashold\n\nN_display = 3\nc = 0\n\nfor idx_arr in similar_vectors:\n    if c == N_display:\n        break\n    if idx_arr[1].size > 0:\n        sim_i = sim_batches[idx_arr[0] // batch_size][idx_arr[0] % batch_size][1] # 1 is most similar to curr el(idx_arr[0])\n        print(idx_arr[0], df_sint[idx_arr[0]]['prompt'].item())\n        print(int(idx_arr[1][0]), df_sint[int(idx_arr[1][0])]['prompt'].item())\n        print(f'Similarity is: {sim_i}', end='\\n\\n')\n        c += 1","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.718196Z","iopub.status.idle":"2024-02-19T23:33:43.718564Z","shell.execute_reply.started":"2024-02-19T23:33:43.718374Z","shell.execute_reply":"2024-02-19T23:33:43.718392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#display most similar prompts which < threashold\n\nN_top = 3\nN_display = 3\nc = 0\n\nfor idx_arr in similar_vectors:\n    if c == N_display:\n        break\n    if idx_arr[1].size == 0:\n        print(idx_arr[0], df_sint[idx_arr[0]]['prompt'].item())\n        print('__'*10)\n        for i in range(1, 1 + N_top):\n            sim_i = sim_batches[idx_arr[0] // batch_size][idx_arr[0] % batch_size][i]\n            idx_i = indices_batches[idx_arr[0] // batch_size][idx_arr[0] % batch_size][i]\n            print(int(idx_i), df_sint[int(idx_i)]['prompt'].item(), sim_i)\n        c += 1\n        print()","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.719571Z","iopub.status.idle":"2024-02-19T23:33:43.719991Z","shell.execute_reply.started":"2024-02-19T23:33:43.719769Z","shell.execute_reply":"2024-02-19T23:33:43.719788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Drop Similarity Data","metadata":{}},{"cell_type":"code","source":"df.to_csv('prompts.csv')","metadata":{"execution":{"iopub.status.busy":"2024-02-19T23:33:43.721453Z","iopub.status.idle":"2024-02-19T23:33:43.721793Z","shell.execute_reply.started":"2024-02-19T23:33:43.721620Z","shell.execute_reply":"2024-02-19T23:33:43.721637Z"},"trusted":true},"execution_count":null,"outputs":[]}]}