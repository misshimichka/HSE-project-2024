{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "from dataclasses import dataclass\n\n\n@dataclass\nclass TrainingConfig:\n    image_size = 128  # the generated image resolution\n    batch_size = 16\n    num_epochs = 100\n    gradient_accumulation_steps = 2\n    learning_rate = 1e-4\n    lr_warmup_steps = 500\n    mixed_precision = 'fp16'  # `no` for float32, `fp16` for automatic mixed precision\n\n    device = \"cuda\"\n    random_state = 42 \n\n\nconfig = TrainingConfig()",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:24.671520Z",
     "iopub.execute_input": "2024-04-20T16:24:24.672140Z",
     "iopub.status.idle": "2024-04-20T16:24:24.683929Z",
     "shell.execute_reply.started": "2024-04-20T16:24:24.672105Z",
     "shell.execute_reply": "2024-04-20T16:24:24.683001Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "import os\nimport random\n\nimport numpy as np\nimport torch\n\n\ndef seed_everything(seed: int,\n                    use_deterministic_algos: bool = False) -> None:\n    \n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.use_deterministic_algorithms(use_deterministic_algos)\n    random.seed(seed)\n    \n   \nseed_everything(config.random_state)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:24.685022Z",
     "iopub.execute_input": "2024-04-20T16:24:24.685342Z",
     "iopub.status.idle": "2024-04-20T16:24:26.393982Z",
     "shell.execute_reply.started": "2024-04-20T16:24:24.685311Z",
     "shell.execute_reply": "2024-04-20T16:24:26.392989Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Utils",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom matplotlib import pyplot as plt\nfrom PIL import Image\n\n\ndef show_images(x):\n    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n    grid = torchvision.utils.make_grid(x)\n    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n    return grid_im\n    \n\n\ndef make_grid(images, size=64):\n    \"\"\"Given a list of PIL images, stack them together into a line for easy viewing\"\"\"\n    output_im = Image.new(\"RGB\", (size * len(images), size))\n    for i, im in enumerate(images):\n        output_im.paste(im.resize((size, size)), (i * size, 0))\n    return output_im",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:26.395212Z",
     "iopub.execute_input": "2024-04-20T16:24:26.395616Z",
     "iopub.status.idle": "2024-04-20T16:24:26.404821Z",
     "shell.execute_reply.started": "2024-04-20T16:24:26.395589Z",
     "shell.execute_reply": "2024-04-20T16:24:26.403802Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torchvision\nfrom datasets import load_dataset\nfrom torchvision import transforms\n\ndataset = load_dataset(\"food101\", split='train')\n\npreprocess = transforms.Compose(\n    [\n        transforms.Resize((config.image_size, config.image_size)),  # Resize\n        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n        transforms.ToTensor(),  # Convert to tensor (0, 1)\n        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n    ]\n)\n\n\ndef transform(examples):\n    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n    return {\"images\": images, \"label\": examples[\"label\"]}\n\n\ndataset.set_transform(transform)\n\ntrain_dataloader = torch.utils.data.DataLoader(\n    dataset, batch_size=config.batch_size, shuffle=True\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:26.407180Z",
     "iopub.execute_input": "2024-04-20T16:24:26.407482Z",
     "iopub.status.idle": "2024-04-20T16:24:31.817145Z",
     "shell.execute_reply.started": "2024-04-20T16:24:26.407459Z",
     "shell.execute_reply": "2024-04-20T16:24:31.815958Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from diffusers import DDPMScheduler\n\nnoise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"linear\")",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:31.818501Z",
     "iopub.execute_input": "2024-04-20T16:24:31.818946Z",
     "iopub.status.idle": "2024-04-20T16:24:31.875168Z",
     "shell.execute_reply.started": "2024-04-20T16:24:31.818918Z",
     "shell.execute_reply": "2024-04-20T16:24:31.874435Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "x = next(iter(train_dataloader))[\"images\"][:1].repeat(8, 1, 1, 1)\ntimesteps = torch.linspace(0, 999, 8).long()\nnoise = torch.randn_like(x)\nnoisy_x = noise_scheduler.add_noise(x, noise, timesteps)\nprint(\"Noisy X shape\", noisy_x.shape)\nshow_images(noisy_x).resize((8 * 128, 128), resample=Image.NEAREST)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:31.876258Z",
     "iopub.execute_input": "2024-04-20T16:24:31.876551Z",
     "iopub.status.idle": "2024-04-20T16:24:32.015910Z",
     "shell.execute_reply.started": "2024-04-20T16:24:31.876526Z",
     "shell.execute_reply": "2024-04-20T16:24:32.014972Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "next(iter(train_dataloader))",
   "metadata": {
    "scrolled": true,
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:32.017058Z",
     "iopub.execute_input": "2024-04-20T16:24:32.017345Z",
     "iopub.status.idle": "2024-04-20T16:24:32.107133Z",
     "shell.execute_reply.started": "2024-04-20T16:24:32.017322Z",
     "shell.execute_reply": "2024-04-20T16:24:32.106183Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from diffusers import UNet2DModel\n\n\nmodel = UNet2DModel(\n    sample_size=config.image_size,  # the target image resolution\n    in_channels=3,  # the number of input channels, 3 for RGB images\n    out_channels=3,  # the number of output channels\n    layers_per_block=2,  # how many ResNet layers to use per UNet block\n    block_out_channels=(128, 128, 256, 256, 512, 512),  # the number of output channes for each UNet block\n    num_class_embeds=102,\n    down_block_types=( \n        \"DownBlock2D\",  # a regular ResNet downsampling block\n        \"DownBlock2D\", \n        \"DownBlock2D\", \n        \"DownBlock2D\", \n        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n        \"DownBlock2D\",\n    ), \n    up_block_types=(\n        \"UpBlock2D\",  # a regular ResNet upsampling block\n        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n        \"UpBlock2D\", \n        \"UpBlock2D\", \n        \"UpBlock2D\", \n        \"UpBlock2D\"  \n      ),\n)",
   "metadata": {
    "scrolled": true,
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:32.108437Z",
     "iopub.execute_input": "2024-04-20T16:24:32.108797Z",
     "iopub.status.idle": "2024-04-20T16:24:33.736932Z",
     "shell.execute_reply.started": "2024-04-20T16:24:32.108763Z",
     "shell.execute_reply": "2024-04-20T16:24:33.736101Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# Let's Trains",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from diffusers import DDPMPipeline\nfrom tqdm.auto import tqdm \n\nnoise_scheduler = DDPMScheduler(\n    num_train_timesteps=1000, \n    beta_schedule=\"linear\"\n)\n\nnoise_scheduler.set_timesteps(num_inference_steps=1000)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:33.738082Z",
     "iopub.execute_input": "2024-04-20T16:24:33.738682Z",
     "iopub.status.idle": "2024-04-20T16:24:33.758513Z",
     "shell.execute_reply.started": "2024-04-20T16:24:33.738647Z",
     "shell.execute_reply": "2024-04-20T16:24:33.757577Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "noise_scheduler.num_inference_steps, noise_scheduler.num_train_timesteps",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:33.759558Z",
     "iopub.execute_input": "2024-04-20T16:24:33.759814Z",
     "iopub.status.idle": "2024-04-20T16:24:33.767603Z",
     "shell.execute_reply.started": "2024-04-20T16:24:33.759790Z",
     "shell.execute_reply": "2024-04-20T16:24:33.766717Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from diffusers.optimization import get_cosine_schedule_with_warmup\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n\nlr_scheduler = get_cosine_schedule_with_warmup(\n    optimizer=optimizer,\n    num_warmup_steps=config.lr_warmup_steps,\n    num_training_steps=(len(train_dataloader) * config.num_epochs),\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:33.768682Z",
     "iopub.execute_input": "2024-04-20T16:24:33.768972Z",
     "iopub.status.idle": "2024-04-20T16:24:33.780095Z",
     "shell.execute_reply.started": "2024-04-20T16:24:33.768948Z",
     "shell.execute_reply": "2024-04-20T16:24:33.779294Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from accelerate import Accelerator\n\naccelerator = Accelerator(\n    mixed_precision=config.mixed_precision,\n    gradient_accumulation_steps=config.gradient_accumulation_steps, \n)\n\ntrain_dataloader, model, optimizer = accelerator.prepare(\n    train_dataloader, model, optimizer\n)",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:33.781356Z",
     "iopub.execute_input": "2024-04-20T16:24:33.782091Z",
     "iopub.status.idle": "2024-04-20T16:24:34.072311Z",
     "shell.execute_reply.started": "2024-04-20T16:24:33.782059Z",
     "shell.execute_reply": "2024-04-20T16:24:34.071466Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def generate(x, model, noise_scheduler, num_inference_steps: int = 1000):\n    model.eval()\n\n    bs = x.shape[0]\n\n    y = torch.randint(\n            0, 102, (bs,), device=config.device\n        ).long()\n    \n    noise_scheduler.set_timesteps(num_inference_steps=num_inference_steps)\n\n    for t in tqdm(noise_scheduler.timesteps):\n        model_input = noise_scheduler.scale_model_input(x, t)\n\n        t_batch = torch.full(\n            size=(bs,), \n            fill_value=t.item(), \n            dtype=torch.long\n        ).cuda()\n\n        with torch.no_grad():\n            noise_pred = model(\n                model_input, \n                t_batch, \n                y,\n                return_dict=False\n            )[0]\n\n        x = noise_scheduler.step(noise_pred, t, x).prev_sample\n\n    return x",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:34.076093Z",
     "iopub.execute_input": "2024-04-20T16:24:34.076473Z",
     "iopub.status.idle": "2024-04-20T16:24:34.083733Z",
     "shell.execute_reply.started": "2024-04-20T16:24:34.076447Z",
     "shell.execute_reply": "2024-04-20T16:24:34.082850Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def add_zero_class(y):\n    bs = y.shape[0]\n\n    y = (y + 1) * (torch.rand((bs,)) >= 0.1).long().to(y.device)\n    return y",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:34.084833Z",
     "iopub.execute_input": "2024-04-20T16:24:34.085105Z",
     "iopub.status.idle": "2024-04-20T16:24:34.098241Z",
     "shell.execute_reply.started": "2024-04-20T16:24:34.085075Z",
     "shell.execute_reply": "2024-04-20T16:24:34.097471Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "losses = []\n\nfor epoch in range(100):\n    for batch in tqdm(train_dataloader):\n        clean_images = batch[\"images\"].to(config.device)\n        labels = add_zero_class(batch[\"label\"]).to(config.device)\n\n        # Sample noise to add to the images\n        noise = torch.randn(clean_images.shape).to(config.device)\n        bs = clean_images.shape[0]\n\n        # Sample a random timestep for each image\n        timesteps = torch.randint(\n            0, noise_scheduler.num_train_timesteps, (bs,), device=config.device\n        ).long()\n\n        # Add noise to the clean images according to the noise magnitude at each timestep\n        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n\n        # Get the model prediction\n        with accelerator.accumulate(model):\n            noise_pred = model(\n                noisy_images, \n                timesteps, \n                labels,\n                return_dict=False\n            )[0]\n    \n            # Calculate the loss\n            loss = F.mse_loss(noise_pred, noise)\n            accelerator.backward(loss)\n            losses.append(loss.item())\n    \n            # Clip gradients and update model parameters\n            if accelerator.sync_gradients:\n                accelerator.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n        \n    if (epoch + 1) % 5 == 0:\n        loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)\n        print(f\"Epoch:{epoch + 1}, loss: {loss_last_epoch}\")\n        torch.save(model, f\"model_{epoch}.pt\")\n",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-20T16:24:34.099431Z",
     "iopub.execute_input": "2024-04-20T16:24:34.099785Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
