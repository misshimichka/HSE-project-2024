{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7440756,"sourceType":"datasetVersion","datasetId":4330767}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Работа с метриками","metadata":{}},{"cell_type":"code","source":"!pip install pyiqa > /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-20T13:58:12.483430Z","iopub.execute_input":"2024-01-20T13:58:12.483919Z","iopub.status.idle":"2024-01-20T13:58:38.524214Z","shell.execute_reply.started":"2024-01-20T13:58:12.483877Z","shell.execute_reply":"2024-01-20T13:58:38.522206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import io\nimport glob\nimport pandas as pd\nfrom tqdm import tqdm\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nfrom transformers import CLIPImageProcessor, CLIPModel, CLIPTokenizer\n\nimport pyiqa\n\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\n\nimport torchvision.transforms as transforms ","metadata":{"execution":{"iopub.status.busy":"2024-01-20T14:21:49.723368Z","iopub.execute_input":"2024-01-20T14:21:49.724729Z","iopub.status.idle":"2024-01-20T14:21:49.737649Z","shell.execute_reply.started":"2024-01-20T14:21:49.724674Z","shell.execute_reply":"2024-01-20T14:21:49.734522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://www.frontiersin.org/articles/10.3389/frai.2022.976235\naes_metric = pyiqa.create_metric(\"laion_aes\")","metadata":{"execution":{"iopub.status.busy":"2024-01-20T13:59:04.651308Z","iopub.execute_input":"2024-01-20T13:59:04.651808Z","iopub.status.idle":"2024-01-20T13:59:38.865818Z","shell.execute_reply.started":"2024-01-20T13:59:04.651763Z","shell.execute_reply":"2024-01-20T13:59:38.864160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cosine_sim(x, y, threshold):\n    return nn.functional.cosine_similarity(x, y) > threshold\n\ndef aesthetic_score(x, threshold):\n    return aes_metric(x) > threshold","metadata":{"execution":{"iopub.status.busy":"2024-01-20T13:59:38.869639Z","iopub.execute_input":"2024-01-20T13:59:38.870215Z","iopub.status.idle":"2024-01-20T13:59:38.877615Z","shell.execute_reply.started":"2024-01-20T13:59:38.870164Z","shell.execute_reply":"2024-01-20T13:59:38.876105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"files = glob.glob(\"/kaggle/input/pickapic-v1-validation/*.parquet\")","metadata":{"execution":{"iopub.status.busy":"2024-01-20T13:59:38.879573Z","iopub.execute_input":"2024-01-20T13:59:38.880130Z","iopub.status.idle":"2024-01-20T13:59:38.897697Z","shell.execute_reply.started":"2024-01-20T13:59:38.880092Z","shell.execute_reply":"2024-01-20T13:59:38.895919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_image_processor = CLIPImageProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\nclip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")","metadata":{"execution":{"iopub.status.busy":"2024-01-20T13:59:38.899698Z","iopub.execute_input":"2024-01-20T13:59:38.900199Z","iopub.status.idle":"2024-01-20T13:59:44.993603Z","shell.execute_reply.started":"2024-01-20T13:59:38.900157Z","shell.execute_reply":"2024-01-20T13:59:44.992237Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PickAPicDataset:\n    def __init__(self, parquet_file):\n        self.data = pd.read_parquet(parquet_file)\n        self.images_1 = self.data[\"jpg_0\"]\n        self.images_2 = self.data[\"jpg_1\"]\n        self.captions = self.data[\"caption\"]\n    \n    def __len__(self):\n        return len(self.captions)\n    \n    def __getitem__(self, idx):\n        img1 = Image.open(io.BytesIO(self.images_1.iloc[idx]))\n        img2 = Image.open(io.BytesIO(self.images_2.iloc[idx]))\n        caption = self.captions.iloc[idx]\n        \n        tensor_img1 = clip_image_processor(img1, return_tensors=\"pt\")\n        tensor_img2 = clip_image_processor(img2, return_tensors=\"pt\")\n        tensor_caption = clip_tokenizer(caption, padding=True, return_tensors=\"pt\", truncation=True, max_length=77)\n        \n        return {\"img1\": tensor_img1, \"img2\": tensor_img2, \"text\": tensor_caption, \"pillow_img1\": img1, \"pillow_img2\": img2, \"caption\": caption}","metadata":{"execution":{"iopub.status.busy":"2024-01-20T15:15:35.052795Z","iopub.execute_input":"2024-01-20T15:15:35.053330Z","iopub.status.idle":"2024-01-20T15:15:35.065972Z","shell.execute_reply.started":"2024-01-20T15:15:35.053292Z","shell.execute_reply":"2024-01-20T15:15:35.064353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"good_ids = []\nbad_ids = []","metadata":{"execution":{"iopub.status.busy":"2024-01-20T15:15:37.256381Z","iopub.execute_input":"2024-01-20T15:15:37.256942Z","iopub.status.idle":"2024-01-20T15:15:37.263757Z","shell.execute_reply.started":"2024-01-20T15:15:37.256901Z","shell.execute_reply":"2024-01-20T15:15:37.262306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Визуализация дитасета","metadata":{}},{"cell_type":"code","source":"threshold_img_img = 0.5\nthreshold_img_text = 0.31\nthreshold_aes = 5\n\nfor file in files:\n    dataset = PickAPicDataset(file)\n    for idx, batch in tqdm(enumerate(dataset)):\n        img1_features = clip_model.get_image_features(**batch[\"img1\"])\n        img2_features = clip_model.get_image_features(**batch[\"img2\"])\n        text_features = clip_model.get_text_features(**batch[\"text\"])\n        \n        if cosine_sim(img1_features, text_features, threshold_img_text) and aesthetic_score(batch[\"img1\"][\"pixel_values\"], threshold_aes):\n            good_ids.append(idx)\n        else:\n            bad_ids.append(idx)\n        \n        if len(good_ids) >= 10 and len(bad_ids) >= 10:\n            break\n    \n    f, axarr = plt.subplots(2, 10, figsize=(100, 10))\n    for i in range(10):\n        axarr[0, i].set_xlabel(dataset[good_ids[i]][\"caption\"])\n        axarr[0, i].imshow(dataset[good_ids[i]][\"pillow_img1\"])\n    for i in range(10):\n        axarr[1, i].imshow(dataset[bad_ids[i]][\"pillow_img1\"])\n    plt.show()\n    break","metadata":{"execution":{"iopub.status.busy":"2024-01-20T15:21:53.010621Z","iopub.execute_input":"2024-01-20T15:21:53.011206Z","iopub.status.idle":"2024-01-20T15:22:05.826940Z","shell.execute_reply.started":"2024-01-20T15:21:53.011164Z","shell.execute_reply":"2024-01-20T15:22:05.819849Z"},"trusted":true},"execution_count":null,"outputs":[]}]}