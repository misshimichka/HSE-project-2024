{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/harlanhong/ICCV2023-MCNET.git\n%cd ICCV2023-MCNET\n!pip install -r requirements.txt\n!pip install gdown\n!pip install  einops torchdiffeq scikit-image==0.18.0\n!pip install imageio[ffmpeg]\n!pip install imageio[pyav]\n!pip install aiogram accelerate peft  git+https://github.com/hukkelas/DSFD-Pytorch-Inference.git git+https://github.com/huggingface/diffusers.git@ee35f1914af802efd4945f47232e8501276d1662","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#%cd ICCV2023-MCNET if restart notebook","metadata":{"execution":{"iopub.status.busy":"2024-05-15T09:58:06.519910Z","iopub.execute_input":"2024-05-15T09:58:06.520674Z","iopub.status.idle":"2024-05-15T09:58:06.534939Z","shell.execute_reply.started":"2024-05-15T09:58:06.520636Z","shell.execute_reply":"2024-05-15T09:58:06.533978Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/working/ICCV2023-MCNET\n","output_type":"stream"}]},{"cell_type":"code","source":"import matplotlib\nmatplotlib.use('Agg')\nimport os, sys\nimport yaml\nfrom argparse import ArgumentParser, Namespace\nfrom tqdm import tqdm\nimport modules.generator as GEN\nimport imageio\nimport numpy as np\nfrom skimage.transform import resize\nfrom skimage import img_as_ubyte\nimport torch\nfrom sync_batchnorm import DataParallelWithCallback\nfrom modules.keypoint_detector import KPDetector\nfrom animate import normalize_kp\nfrom scipy.spatial import ConvexHull\nfrom collections import OrderedDict\nimport pdb\nif sys.version_info[0] < 3:\n    raise Exception(\"You must use Python 3 or higher. Recommended version is Python 3.7\")\n\ndef load_checkpoints(config_path, checkpoint_path, cpu=False):\n\n    with open(config_path) as f:\n        config = yaml.load(f, Loader=yaml.Loader)\n    if opt.kp_num != -1:\n        config['model_params']['common_params']['num_kp'] = opt.kp_num\n    generator = getattr(GEN, opt.generator)(**config['model_params']['generator_params'],**config['model_params']['common_params'],**{'mbunit':opt.mbunit,'mb_spatial':opt.mb_spatial,'mb_channel':opt.mb_channel})\n    if not cpu:\n        generator.cuda()\n    kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n                             **config['model_params']['common_params'])\n    if not cpu:\n        kp_detector.cuda()\n    \n    if cpu:\n        checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n    else:\n        checkpoint = torch.load(checkpoint_path,map_location=\"cuda:0\")\n    \n    ckp_generator = OrderedDict((k.replace('module.',''),v) for k,v in checkpoint['generator'].items())\n    generator.load_state_dict(ckp_generator)\n    ckp_kp_detector = OrderedDict((k.replace('module.',''),v) for k,v in checkpoint['kp_detector'].items())\n    kp_detector.load_state_dict(ckp_kp_detector)\n    \n    if not cpu:\n        generator = DataParallelWithCallback(generator)\n        kp_detector = DataParallelWithCallback(kp_detector)\n\n    generator.eval()\n    kp_detector.eval()\n    \n    return generator, kp_detector\n\n\ndef make_animation(source_image, driving_video, generator, kp_detector, relative=True, adapt_movement_scale=True, cpu=False):\n    sources = []\n    drivings = []\n    with torch.no_grad():\n        predictions = []\n        source = torch.tensor(source_image[np.newaxis].astype(np.float32)).permute(0, 3, 1, 2)\n        if not cpu:\n            source = source.cuda()\n        driving = torch.tensor(np.array(driving_video)[np.newaxis].astype(np.float32)).permute(0, 4, 1, 2, 3)\n\n        kp_source = kp_detector(source)\n        if not cpu:\n            kp_driving_initial = kp_detector(driving[:, :, 0].cuda())\n        else:\n            kp_driving_initial = kp_detector(driving[:, :, 0])\n        for frame_idx in tqdm(range(driving.shape[2])):\n            driving_frame = driving[:, :, frame_idx]\n            if not cpu:\n                driving_frame = driving_frame.cuda()\n            kp_driving = kp_detector(driving_frame)\n            kp_norm = normalize_kp(kp_source=kp_source, kp_driving=kp_driving,\n                                   kp_driving_initial=kp_driving_initial, use_relative_movement=relative,\n                                   use_relative_jacobian=relative, adapt_movement_scale=adapt_movement_scale)\n            out = generator(source, kp_source=kp_source, kp_driving=kp_norm)\n            drivings.append(np.transpose(driving_frame.data.cpu().numpy(), [0, 2, 3, 1])[0])\n            sources.append(np.transpose(source.data.cpu().numpy(), [0, 2, 3, 1])[0])\n            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n    return sources, drivings, predictions\n\nopt = Namespace(config='config/vox-256.yaml',\n                checkpoint='/kaggle/input/checkpoint/00000099-checkpoint.pth.tar',\n                source_image='img.jpg',\n                driving_video='output.mp4',\n                result_video='result.mp4',\n                relative=True,\n                adapt_scale=True,\n                generator='Unet_Generator_keypoint_aware',\n                kp_num=15,\n                mb_channel=512,\n                mb_spatial=32,\n                mbunit='ExpendMemoryUnit',\n                memsize=1,\n                find_best_frame=False,\n                best_frame=None,\n                cpu=False)\n\ngenerator, kp_detector = load_checkpoints(config_path=opt.config, checkpoint_path=opt.checkpoint, cpu=opt.cpu)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T09:58:06.698521Z","iopub.execute_input":"2024-05-15T09:58:06.698825Z","iopub.status.idle":"2024-05-15T09:58:27.569163Z","shell.execute_reply.started":"2024-05-15T09:58:06.698798Z","shell.execute_reply":"2024-05-15T09:58:27.568292Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /usr/local/src/pytorch/aten/src/ATen/native/TensorShape.cpp:3526.)\n  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n","output_type":"stream"}]},{"cell_type":"code","source":"def generate_animation(source_image, driving_video, output_video):    \n    source_image = imageio.imread(source_image)\n    reader = imageio.get_reader(driving_video)\n    fps = reader.get_meta_data()['fps']\n    driving_video = []\n    try:\n        for im in reader:\n            driving_video.append(im)\n    except RuntimeError:\n        pass\n    reader.close()\n\n    source_image = resize(source_image, (256, 256))[..., :3]\n    driving_video = [resize(frame, (256, 256))[..., :3] for frame in driving_video]\n\n    sources, drivings, predictions = make_animation(source_image, driving_video, generator, kp_detector, relative=opt.relative, adapt_movement_scale=opt.adapt_scale, cpu=opt.cpu)\n    imageio.mimsave(output_video, [img_as_ubyte(p) for p in predictions], fps=fps)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:09:05.926346Z","iopub.execute_input":"2024-05-15T10:09:05.927067Z","iopub.status.idle":"2024-05-15T10:09:05.934466Z","shell.execute_reply.started":"2024-05-15T10:09:05.927033Z","shell.execute_reply":"2024-05-15T10:09:05.933497Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfrom io import BytesIO\nimport os\nimport logging\nimport cv2\nimport face_detection\nimport numpy as np\nimport asyncio\nfrom collections import deque\n\nfrom aiogram.types import FSInputFile, InputFile\nfrom aiogram.enums import ParseMode\nfrom aiogram import Bot, Dispatcher, Router, types\nfrom aiogram.filters import CommandStart, Command\nfrom aiogram import F\n\nimport torch\nfrom diffusers import StableDiffusionInstructPix2PixPipeline, LCMScheduler\n\n\nphoto_storage = {}\n\n\ndef load_pix2pix_model(path, device):\n    pipeline = StableDiffusionInstructPix2PixPipeline.from_pretrained(\n        path,\n        torch_dtype=torch.float16,\n        safety_checker=None,\n        # local_files_only=True,\n        # cache_dir=mode\n    )\n\n    pipeline.scheduler = LCMScheduler.from_config(pipeline.scheduler.config)\n\n    pipeline.load_lora_weights(\n        pretrained_model_name_or_path_or_dict=\"latent-consistency/lcm-lora-sdv1-5\",\n        weight_name=\"pytorch_lora_weights.safetensors\",\n        # cache_dir=\"lcm\",\n        # local_files_only=True\n    )\n\n    pipeline.generator = torch.Generator(device=device).manual_seed(42)\n\n    pipeline.load_ip_adapter(\n        pretrained_model_name_or_path_or_dict=\"h94/IP-Adapter\",\n        subfolder=\"models\",\n        weight_name=\"ip-adapter_sd15.bin\",\n        # local_files_only=True,\n        # cache_dir=\"adapter\"\n    )\n    pipeline.set_ip_adapter_scale(1)\n\n    pipeline = pipeline.to(device)\n    return pipeline\n    \n\n\nmodel_flowers_id = \"misshimichka/pix2pix_people_flowers_v2\"\nmodel_cat_id = \"misshimichka/pix2pix_cat_ears\"\nmodel_clown_id = \"misshimichka/pix2pix_clown_faces\"\nmodel_butterfly_id = \"misshimichka/pix2pix_butterflies\"\nmodel_pink_id = \"misshimichka/pix2pix_pink_hair\"\nmodel_id = \"misshimichka/instructPix2PixCartoon_4860_ckpt\"\n\n\n    \nmodels = {\n    \"default\": load_pix2pix_model(model_id, 'cuda:0'),\n    \"flowers\": load_pix2pix_model(model_flowers_id, 'cuda:0'),\n    \"cat\": load_pix2pix_model(model_cat_id, 'cuda:0'),\n    \"butterfly\": load_pix2pix_model(model_butterfly_id, 'cuda:1'),\n    \"clown\": load_pix2pix_model(model_clown_id, 'cuda:1'),\n    \"pink\": load_pix2pix_model(model_pink_id, 'cuda:1')\n}\n\ndetector = face_detection.build_detector(\n  \"DSFDDetector\", confidence_threshold=.5, nms_iou_threshold=.3)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T09:58:27.580360Z","iopub.execute_input":"2024-05-15T09:58:27.580654Z","iopub.status.idle":"2024-05-15T10:00:50.096888Z","shell.execute_reply.started":"2024-05-15T09:58:27.580620Z","shell.execute_reply":"2024-05-15T10:00:50.096017Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"2024-05-15 09:58:33.467607: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-15 09:58:33.467664: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-15 09:58:33.474717: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"601062d4570545eeb1d878959c0e3b53"}},"metadata":{}},{"name":"stderr","text":"The config attributes {'algorithm_type': 'dpmsolver++', 'lower_order_final': True, 'skip_prk_steps': True, 'solver_order': 2, 'solver_type': 'midpoint'} were passed to LCMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cc5af67b54b4c87b8c49abbfd139182"}},"metadata":{}},{"name":"stderr","text":"The config attributes {'algorithm_type': 'dpmsolver++', 'lower_order_final': True, 'skip_prk_steps': True, 'solver_order': 2, 'solver_type': 'midpoint'} were passed to LCMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c82dbc03f4dd4b1cbfe630963ddafa00"}},"metadata":{}},{"name":"stderr","text":"The config attributes {'algorithm_type': 'dpmsolver++', 'lower_order_final': True, 'skip_prk_steps': True, 'solver_order': 2, 'solver_type': 'midpoint'} were passed to LCMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"351a6c8d3a2347c296d243e503939e18"}},"metadata":{}},{"name":"stderr","text":"The config attributes {'algorithm_type': 'dpmsolver++', 'lower_order_final': True, 'skip_prk_steps': True, 'solver_order': 2, 'solver_type': 'midpoint'} were passed to LCMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bdff62847e244b098abc2f2e2420277"}},"metadata":{}},{"name":"stderr","text":"The config attributes {'algorithm_type': 'dpmsolver++', 'lower_order_final': True, 'skip_prk_steps': True, 'solver_order': 2, 'solver_type': 'midpoint'} were passed to LCMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0417dc5618b14bb7952813eb9f13f375"}},"metadata":{}},{"name":"stderr","text":"The config attributes {'algorithm_type': 'dpmsolver++', 'lower_order_final': True, 'skip_prk_steps': True, 'solver_order': 2, 'solver_type': 'midpoint'} were passed to LCMScheduler, but are not expected and will be ignored. Please verify your scheduler_config.json configuration file.\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"!gdown 1dk973WGzD7n9NlIw3cl6J4bqHKObgPs2","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:13:20.330174Z","iopub.execute_input":"2024-05-15T10:13:20.331395Z","iopub.status.idle":"2024-05-15T10:13:22.878447Z","shell.execute_reply.started":"2024-05-15T10:13:20.331340Z","shell.execute_reply":"2024-05-15T10:13:22.877174Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1dk973WGzD7n9NlIw3cl6J4bqHKObgPs2\nTo: /kaggle/working/ICCV2023-MCNET/wow-grey.mp4\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 295k/295k [00:00<00:00, 107MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"def image_grid(imgs, rows, cols):\n    assert len(imgs) == rows * cols\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(cols*w, rows*h))\n    for i, img in enumerate(imgs):\n        grid.paste(img, box=(i%cols*w, i//cols*h))\n    return grid\n\ndef crop_img(im):\n  if isinstance(im, Image.Image):\n    im = cv2.cvtColor(np.array(im), cv2.COLOR_RGB2BGR)\n  elif isinstance(im, str) and os.path.exists(im):\n    im = cv2.imread(im)\n    im = cv2.resize(im, (512, 512))\n    im = im[:, :, ::-1]\n  else:\n    return None\n\n  detections = detector.detect(im)\n\n  if detections.shape[0] != 1:\n    return None\n  xmin, ymin, xmax, ymax, _ = [int(i) + 1 for i in detections.tolist()[0]]\n  ymin = max(0, ymin - 50)\n  ymax = min(512, ymax + 50)\n  xmin = max(0, xmin - 50)\n  xmax = min(xmax + 50, 512)\n  cropped_img = im[ymin:ymax, xmin:xmax]\n\n  im_pil = Image.fromarray(cropped_img)\n  img = im_pil.resize((512, 512))\n  return img\n\n\ndef generate(original_image, mode):\n    print(\"Generating image...\")\n\n    cropped_image = crop_img('our_img.jpg')\n\n    if not cropped_image:\n        return None\n\n    edited_image = models[mode](\n        prompt=\"Refashion the photo into a sticker.\",\n        image=cropped_image,\n        ip_adapter_image=cropped_image,\n        num_inference_steps=4,\n        image_guidance_scale=1,\n        guidance_scale=2,\n        num_images_per_prompt=3\n    ).images\n    \n    torch.cuda.empty_cache()\n\n    \n    for idx, img in enumerate(edited_image):\n      img.save(f\"result{idx}.webp\", \"webp\")\n\n    return image_grid(edited_image, 3, 1)\n\n\ndef get_styles_markup():\n    default_btn = types.InlineKeyboardButton(text=\"Default ü§´üßè‚Äç\", callback_data=\"default\")\n    flowers_btn = types.InlineKeyboardButton(text=\"Flowers üå∏üå∫\", callback_data=\"flowers\")\n    cat_btn = types.InlineKeyboardButton(text=\"Cat ears üêàüê±\", callback_data=\"cat\")\n    butterfly_btn = types.InlineKeyboardButton(text=\"Butterflies ü¶ãüåà\", callback_data=\"butterfly\")\n    clown_btn = types.InlineKeyboardButton(text=\"Clown ü§°ü§£\", callback_data=\"clown\")\n    pink_btn = types.InlineKeyboardButton(text=\"Pink hair ü©∑‚ú®\", callback_data=\"pink\")\n    animate_btn = types.InlineKeyboardButton(text=\"Animate 1Ô∏è‚É£4Ô∏è‚É£8Ô∏è‚É£8Ô∏è‚É£\", callback_data=\"animate\")\n    markup = types.InlineKeyboardMarkup(\n        inline_keyboard=[[default_btn, flowers_btn],\n                         [cat_btn, butterfly_btn],\n                         [clown_btn, pink_btn],\n                        [animate_btn]]\n    )\n    return markup\n\nasync def handle_selection(message: types.Message):\n    index = int(message.text) - 1\n    chat_id = message.chat.id\n    await bot.send_sticker(\n                chat_id=chat_id,\n                sticker=FSInputFile(f\"result{index}.webp\"),\n                emoji=\"üéÅ\",\n            )\n    await bot.send_photo(chat_id, photo=FSInputFile(path=f\"result{index}.webp\"), caption=\"1488\")\n\n\nasync def handle_start(message: types.Message):\n    await message.reply(\"Welcome to Stickerify bot! ü•∂\\nSend me a photo and I will create your own sticker üë†\")\n\n\nasync def handle_photo(message: types.Message):\n    photo = message.photo[-1]\n    file_id = photo.file_id\n    chat_id = message.chat.id\n    if chat_id not in photo_storage.keys():\n        photo_storage[chat_id] = deque()\n    photo_storage[chat_id].append(file_id)\n\n    await message.reply(\"Choose your sticker style:\", reply_markup=get_styles_markup())\n\n\nasync def handle_debug(message: types.Message):\n    print(photo_storage)\n\n\nasync def process_stickerify_callback(callback_query: types.CallbackQuery):\n    chat_id = callback_query.from_user.id\n    sticker_style = callback_query.data\n    print(sticker_style)\n    if chat_id in photo_storage.keys() and len(photo_storage[chat_id]) > 0:\n        file_id = photo_storage[chat_id].popleft()\n        try:\n            file = await bot.get_file(file_id)\n            file_path = file.file_path\n            contents = await bot.download_file(file_path)\n\n            img = Image.open(BytesIO(contents.getvalue()))\n            img.save('our_img.jpg')\n\n            if sticker_style != 'animate':\n                await bot.send_message(chat_id, \"Started generating your sticker! üë®‚Äçüî¨\")\n                stickerified_images = generate(img, sticker_style)\n                if not stickerified_images:\n                    await bot.send_message(chat_id, \"Unfortunately, we couldn't find a human face on your \"\n                                                    \"photo, or there were too many of them üò∞ Please, \"\n                                                    \"send another photo.\")\n                    return\n\n\n\n                stickerified_images.save(f\"{chat_id}_result.jpeg\")\n                await bot.send_photo(chat_id, photo=FSInputFile(path=f\"{chat_id}_result.jpeg\"), caption=\"Type number from 1 to 9 to pick up sticker.\")\n            else:\n                await bot.send_message(chat_id, \"Started animating your sticker! üë®‚Äçüî¨\")\n                generate_animation('our_img.jpg', 'wow-grey.mp4', 'result.mp4')\n                torch.cuda.empty_cache()\n                await bot.send_video(chat_id, FSInputFile('result.mp4'))\n                \n\n\n        except Exception as e:\n            print(e)\n            await bot.send_message(chat_id, f\"Sorry, an error occurred.\\n{e}\")\n\n    else:\n        await bot.send_message(chat_id, \"We couldn't find your photo. Please send it again.\")\n\n\ndef setup_handlers(router: Router):\n    router.message.register(handle_selection, F.text.lower().in_(['1', '2', '3', '4', '5', '6', '7', '8', '9']))\n    router.message.register(handle_start, CommandStart())\n    router.message.register(handle_photo, F.content_type.in_({'photo'}))\n    router.message.register(handle_debug, Command(\"debug\"))\n    router.callback_query.register(process_stickerify_callback)\n\n\nbot = Bot(\"5818667076:AAEJw19A7hTV6XjZOu38SYS3T9Jm8ZOicZ8\", parse_mode=ParseMode.HTML)\n\n\nasync def main():\n    router = Router()\n    setup_handlers(router)\n\n    dispatcher = Dispatcher()\n    dispatcher.include_router(router)\n    await dispatcher.start_polling(bot)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:22:46.439752Z","iopub.execute_input":"2024-05-15T10:22:46.440152Z","iopub.status.idle":"2024-05-15T10:22:46.482254Z","shell.execute_reply.started":"2024-05-15T10:22:46.440120Z","shell.execute_reply":"2024-05-15T10:22:46.481233Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_1223/784853933.py:159: DeprecationWarning: Passing `parse_mode`, `disable_web_page_preview` or `protect_content` to Bot initializer is deprecated. This arguments will be removed in 3.7.0 version\nUse `default=DefaultBotProperties(...)` instead.\n  bot = Bot(\"5818667076:AAEJw19A7hTV6XjZOu38SYS3T9Jm8ZOicZ8\", parse_mode=ParseMode.HTML)\n","output_type":"stream"}]},{"cell_type":"code","source":"await main()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T10:22:47.172554Z","iopub.execute_input":"2024-05-15T10:22:47.173013Z","iopub.status.idle":"2024-05-15T10:30:14.250653Z","shell.execute_reply.started":"2024-05-15T10:22:47.172957Z","shell.execute_reply":"2024-05-15T10:30:14.249633Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"animate\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_1223/3942548873.py:2: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n  source_image = imageio.imread(source_image)\n100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26/26 [00:03<00:00,  8.04it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}